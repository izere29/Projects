{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e8db22",
   "metadata": {},
   "source": [
    "# capstone Project : Prediction of airbnb listing price \n",
    "\n",
    "## Overview \n",
    "\n",
    "This project aims to build a regression model that predict the listing price of an airbnb and also identifier the features that are needed to maximize the price.\n",
    "The airbnb is a booming business in the hospitality industry right now . Many people are buying house or renting house to make them airbnbs and increase their income. The aim of this project is to build a regression machine learning model that can help in predicting the price of the airbnb and helping the owner maximize the profits.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "I got this dataset from Kaggle.com but it is a combination of different dataset from inside Airbnb.com.It is collection of some listing of houses in different city of United States.https://www.kaggle.com/kritikseth/us-airbnb-open-data\n",
    "\n",
    "This dataset has one file- AB_US_2020.csv which has columns describing features such as host id, hostname, listing id, listing name, latitude and longitude of listing, the neighbourhood, price, room type, minimum number of nights, number of reviews, last review date, reviews per month, availability, host listings and city.\n",
    "\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "In this stage , we are did some data cleanning , removed some empty rows,combined the title and self text in one column to facilitate the using the Nature Language Processing wich will hep to change the text into numerical features for modelling.\n",
    "\n",
    "In order to understand the and analyse the txt data we looked ata the distribution o the word count and the length of the post We look into the distribution of the overall length of the posts and word counts , the distribution is right skewed which means that the most post are between 0-200 words or between 0-1000 characters for both subreddit.\n",
    "\n",
    "In the EDA we also looked at the most commonly used words overall and also in the respective subreddit.We used different method to analyse the word from the post .The countvectorizer which count the occurrence of the word in our dataset and TF-IDF vectorizer means Term Frequency - Inverse Document Frequency. This is a statistic that is based on the frequency of a word in the dataset but it also provides a numerical representation of how important a word is for statistical analysis.\n",
    "\n",
    "Using these methods we also look at the combination of 2 words .\n",
    "\n",
    "## Modeling\n",
    "\n",
    "For the modelling part , we start by making a train -test split with train size of 70% of the dataframe. We are going to fit different models to compare them  to see which model serve well our purpose.\n",
    "\n",
    "We decided to use the Multimonial Naives Bayes, Random forest and a KNN.After instantiating the model we use a confusion matrix to look at the True positive, True Negative, False Pasitive and False negative. The metrics we used are Recall means from all the positive classes, how many we predicted correctly. Precision means from all the classes we have predicted as positive, how many are actually positive. Accuracy means From all the classes (positive and negative), how many of them we have predicted correctly. We will also use F1_score which is a good compromise for the precision and recall. The high they are the better.\n",
    "\n",
    "## Evaluation\n",
    "Using the Random Forest Classifier model\n",
    "The model has an Accuracy score of 99% on training data and 87% on unseen data which is an indication of high variance.It also means on data that the model trainned on it capable of classifiying correctly 99% of the posts into sustainability or urban planning but perform on 87% on new posts.\n",
    "\n",
    "Precision is 85% which means that for all the posts the model predicted to be from sustainability 85% are actually from the sustainability posts .\n",
    "\n",
    "The recall is 91% which means that from all the posts we predicted 91% correctly both from sustainability and urban planning. The f-1 score which is a good compromise for the precision and recall is 88%.\n",
    "\n",
    "Using the Multinomial Naives Bayes model which look into probabilities The model has A score of 94% on training data and 90% on unseen data which is an indication of high variance but less than the one from Random forest .\n",
    "\n",
    "Precision is 90% which means that for all the posts the model predicted to be from sustainability 90% are actually from the sustainability posts . The recall is 89% which means that from all the posts we predicted 89 % correctly both from sustainability and urban planning. The f-1 score is which is a good compromise for the precision and recall 90%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
